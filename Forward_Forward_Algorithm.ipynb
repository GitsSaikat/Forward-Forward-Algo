{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "CtIeCnVezKLf"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZKCHFCrFzLRF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_forward(model, X, y):\n",
        "    # Split the input data into two streams\n",
        "    X_pos = X[y == 1]\n",
        "    X_neg = X[y == 0]\n",
        "    \n",
        "    # Check if either tensor is empty\n",
        "    if X_pos.nelement() == 0 or X_neg.nelement() == 0:\n",
        "        return torch.tensor(0.0)\n",
        "    \n",
        "    # Compute the forward pass of the model on both streams of data\n",
        "    out_pos = model(X_pos)\n",
        "    out_neg = model(X_neg)\n",
        "    \n",
        "    # Compute the loss\n",
        "    loss = torch.mean((out_pos - out_neg)**2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "46-aVkgJzPsQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCGhcsEEzTme",
        "outputId": "d6c595c0-762a-4a19-ba73-9381caf6317d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 29307781.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and optimizer\n",
        "model = Net()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "wM7XwS5izYSj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model using the Forward-Forward Algorithm\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        loss = forward_forward(model, inputs, labels)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch: {epoch} Loss: {running_loss / len(trainloader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pYuvHsczbmq",
        "outputId": "a3a5a3f0-63fe-4589-96e2-cc662397e8e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 2.9085383558231116e-06\n",
            "Epoch: 1 Loss: 3.0060529933689396e-06\n",
            "Epoch: 2 Loss: 3.0055480017108493e-06\n",
            "Epoch: 3 Loss: 3.0801914619951278e-06\n",
            "Epoch: 4 Loss: 3.0712073797985794e-06\n",
            "Epoch: 5 Loss: 2.834570900449762e-06\n",
            "Epoch: 6 Loss: 2.9115084275690606e-06\n",
            "Epoch: 7 Loss: 2.9461141034062167e-06\n",
            "Epoch: 8 Loss: 2.903673986929789e-06\n",
            "Epoch: 9 Loss: 2.891479670179251e-06\n",
            "Epoch: 10 Loss: 3.0996296259945667e-06\n",
            "Epoch: 11 Loss: 2.8977382081211544e-06\n",
            "Epoch: 12 Loss: 3.0660015215107705e-06\n",
            "Epoch: 13 Loss: 3.1314957226459227e-06\n",
            "Epoch: 14 Loss: 2.8681984919057866e-06\n",
            "Epoch: 15 Loss: 2.8208324821116547e-06\n",
            "Epoch: 16 Loss: 2.9298895262854785e-06\n",
            "Epoch: 17 Loss: 2.9589574939564046e-06\n",
            "Epoch: 18 Loss: 2.930693306370813e-06\n",
            "Epoch: 19 Loss: 2.925326023632806e-06\n",
            "Epoch: 20 Loss: 2.9053859716441366e-06\n",
            "Epoch: 21 Loss: 2.8297916074279784e-06\n",
            "Epoch: 22 Loss: 3.0158187213601195e-06\n",
            "Epoch: 23 Loss: 2.9126313975393712e-06\n",
            "Epoch: 24 Loss: 2.9123269974661523e-06\n",
            "Epoch: 25 Loss: 3.1293613135676423e-06\n",
            "Epoch: 26 Loss: 3.003208406616977e-06\n",
            "Epoch: 27 Loss: 2.855917443557701e-06\n",
            "Epoch: 28 Loss: 3.045435758685926e-06\n",
            "Epoch: 29 Loss: 2.9287028972339613e-06\n",
            "Epoch: 30 Loss: 3.099888188226032e-06\n",
            "Epoch: 31 Loss: 3.0666796881268964e-06\n",
            "Epoch: 32 Loss: 2.997179424728529e-06\n",
            "Epoch: 33 Loss: 2.916114079853287e-06\n",
            "Epoch: 34 Loss: 3.1006704657920636e-06\n",
            "Epoch: 35 Loss: 2.993179929453618e-06\n",
            "Epoch: 36 Loss: 2.797021022106492e-06\n",
            "Epoch: 37 Loss: 3.1151151170342927e-06\n",
            "Epoch: 38 Loss: 2.9885399065096864e-06\n",
            "Epoch: 39 Loss: 2.8202846480598964e-06\n",
            "Epoch: 40 Loss: 3.0350604472187117e-06\n",
            "Epoch: 41 Loss: 3.0363951659637676e-06\n",
            "Epoch: 42 Loss: 2.936274495577891e-06\n",
            "Epoch: 43 Loss: 2.9416711701833263e-06\n",
            "Epoch: 44 Loss: 2.996584211141453e-06\n",
            "Epoch: 45 Loss: 2.97514954707367e-06\n",
            "Epoch: 46 Loss: 2.986059594168182e-06\n",
            "Epoch: 47 Loss: 2.961597887406242e-06\n",
            "Epoch: 48 Loss: 2.819896271266771e-06\n",
            "Epoch: 49 Loss: 3.0443903124887584e-06\n",
            "Epoch: 50 Loss: 3.0434876496565265e-06\n",
            "Epoch: 51 Loss: 2.9358811785186843e-06\n",
            "Epoch: 52 Loss: 2.90949900258056e-06\n",
            "Epoch: 53 Loss: 2.87989622056557e-06\n",
            "Epoch: 54 Loss: 2.9410487787936292e-06\n",
            "Epoch: 55 Loss: 2.8181919234884843e-06\n",
            "Epoch: 56 Loss: 2.8701662753883284e-06\n",
            "Epoch: 57 Loss: 3.0809340322230126e-06\n",
            "Epoch: 58 Loss: 2.981321403713082e-06\n",
            "Epoch: 59 Loss: 2.8617730959922485e-06\n",
            "Epoch: 60 Loss: 3.1565482529367726e-06\n",
            "Epoch: 61 Loss: 2.9932893423665517e-06\n",
            "Epoch: 62 Loss: 2.8663759775736253e-06\n",
            "Epoch: 63 Loss: 2.9083770156830725e-06\n",
            "Epoch: 64 Loss: 2.8274550395326513e-06\n",
            "Epoch: 65 Loss: 3.0336541987981036e-06\n",
            "Epoch: 66 Loss: 2.974415427306667e-06\n",
            "Epoch: 67 Loss: 2.964910642349423e-06\n",
            "Epoch: 68 Loss: 2.8229760232898118e-06\n",
            "Epoch: 69 Loss: 2.944348633009213e-06\n",
            "Epoch: 70 Loss: 2.947663382947212e-06\n",
            "Epoch: 71 Loss: 2.965667762873636e-06\n",
            "Epoch: 72 Loss: 2.9800980006075407e-06\n",
            "Epoch: 73 Loss: 2.9943372440584426e-06\n",
            "Epoch: 74 Loss: 2.9090586611346224e-06\n",
            "Epoch: 75 Loss: 2.955915566471958e-06\n",
            "Epoch: 76 Loss: 2.875176615343662e-06\n",
            "Epoch: 77 Loss: 2.9733918376768997e-06\n",
            "Epoch: 78 Loss: 3.1263513582780434e-06\n",
            "Epoch: 79 Loss: 2.930330651006443e-06\n",
            "Epoch: 80 Loss: 2.9836570770930846e-06\n",
            "Epoch: 81 Loss: 2.9732847858576862e-06\n",
            "Epoch: 82 Loss: 2.7704314133916343e-06\n",
            "Epoch: 83 Loss: 2.8301258560895805e-06\n",
            "Epoch: 84 Loss: 2.9042245477285177e-06\n",
            "Epoch: 85 Loss: 2.9137187409833133e-06\n",
            "Epoch: 86 Loss: 2.949362906438182e-06\n",
            "Epoch: 87 Loss: 2.994807028098876e-06\n",
            "Epoch: 88 Loss: 2.9238740630353277e-06\n",
            "Epoch: 89 Loss: 2.940666193935613e-06\n",
            "Epoch: 90 Loss: 3.031846269796006e-06\n",
            "Epoch: 91 Loss: 2.8515843947934626e-06\n",
            "Epoch: 92 Loss: 2.997667122344865e-06\n",
            "Epoch: 93 Loss: 2.9322931648857776e-06\n",
            "Epoch: 94 Loss: 2.897762185057218e-06\n",
            "Epoch: 95 Loss: 3.0313934333025827e-06\n",
            "Epoch: 96 Loss: 2.9256722419086146e-06\n",
            "Epoch: 97 Loss: 2.9325263129067026e-06\n",
            "Epoch: 98 Loss: 3.041452209704403e-06\n",
            "Epoch: 99 Loss: 2.9727627439388016e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test dataset\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "6w4yf_Ow1ozy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test dataset\n",
        "pos_distances = []\n",
        "neg_distances = []\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        pos_outputs = outputs[labels == 1]\n",
        "        neg_outputs = outputs[labels == 0]\n",
        "        if pos_outputs.nelement() > 0 and neg_outputs.nelement() > 0:\n",
        "            pos_distance = torch.mean(torch.cdist(pos_outputs, pos_outputs))\n",
        "            neg_distance = torch.mean(torch.cdist(neg_outputs, neg_outputs))\n",
        "            pos_distances.append(pos_distance.item())\n",
        "            neg_distances.append(neg_distance.item())\n",
        "\n",
        "print(f'Average positive distance: {np.mean(pos_distances)}')\n",
        "print(f'Average negative distance: {np.mean(neg_distances)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPvtUwGPCC4x",
        "outputId": "5777b62d-899d-4fa4-b5a4-187c8b51a30d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average positive distance: 0.0011169865735218116\n",
            "Average negative distance: 0.0009342479406540364\n"
          ]
        }
      ]
    }
  ]
}